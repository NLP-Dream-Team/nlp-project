{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc733dd",
   "metadata": {},
   "source": [
    "# Predicting Languages with READMEs\n",
    "\n",
    "Rajaram Gautam   \n",
    "Mason Sherbondy   \n",
    "Sophia Stewart   \n",
    "Joshua Wheeler\n",
    "\n",
    "February 11, 2022\n",
    "\n",
    "---\n",
    "## Executive Summary\n",
    "The goal of this project is to determine what language is primarily used in a repository based only on the contents of that repository's README file.\n",
    "\n",
    "---\n",
    "## Data Wrangling\n",
    "\n",
    "We decided to use repositories that came up in a GitHub search for the word 'zombie'. We acquired the data using Zach Gulde's functions for acquiring data using GitHub's API as well as some functions defined by our group to acquire the names of the repositories. To prepare our data, we removed those repositories which had non-English characters. We then normalized, tokenized, stemmed, lemmatized, and removed stopwords from each README. Before moving on to our data exploration, we split our dataset into train, validate, and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b0ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acquire import get_df\n",
    "from prepare import remove_stopwords, prep_repo_data, split_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import explore\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52791918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve dataframe from acquire\n",
    "df = get_df()\n",
    "\n",
    "# prepare df\n",
    "df = prep_repo_data(df)\n",
    "\n",
    "# split data\n",
    "train, validate, test = split_data(df)\n",
    "\n",
    "# add exploration columns\n",
    "df, train, validate, test = explore.explore(df,train,validate,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d67a7f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Question 1: What is the distribution of our Inverse Document Frequencies for the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributions_grid(df, quant_vars):\n",
    "\n",
    "    '''\n",
    "    This function creates a nice sized figure, enumerates the list of features passed into the function, creates a grid of subplots,\n",
    "    and then charts histograms for features in the list onto the subplots.\n",
    "    '''\n",
    "\n",
    "    plt.figure(figsize = (11, 8))    # create figure\n",
    "\n",
    "    for i, cat in enumerate(quant_vars[:6]):    # loop through enumerated list\n",
    "        plot_number = i + 1     # i starts at 0, but plot nos should start at 1\n",
    "        plt.subplot(2, 3, plot_number)    # create subplot\n",
    "        plt.title(cat)    # title \n",
    "        plt.ylabel('Count')    # set y-axis label\n",
    "        plt.xlabel('Inverse Document Frequency')    # set x-axis label\n",
    "        df[cat].hist(color = 'teal', edgecolor = 'darkslategrey')   # display histogram for column\n",
    "        plt.grid(False)    # rid grid-lines\n",
    "        plt.tight_layout();    # clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55899580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf distributions\n",
    "dist = explore.get_idf_dist(train)\n",
    "distributions_grid(train, dist[:6])\n",
    "print('IDF Distributions for Top 6 Occuring Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5522f8",
   "metadata": {},
   "source": [
    "### Answer 1: The majority of our top words do not have a distribution that is helpful, as most of these values are 0.\n",
    "---\n",
    "### Question 2: Does the length of the README vary by programming language?\n",
    "\n",
    "#### $H_{0}$: Average message length for languages is about the same.\n",
    "#### $H_{a}$: Average message length between languages is significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4172ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kruskal-wallis test for README length\n",
    "explore.run_kruskal_wallis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fb83c",
   "metadata": {},
   "source": [
    "### Answer 2: There is a significant difference in average message length between languages.\n",
    "---\n",
    "### Question 3: Do different programming languages use a different number of unique words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545afc0",
   "metadata": {},
   "source": [
    "- Javascript: AWS, stack, response, topic, gateway, endpoint  \n",
    "- HTML: integration, workshop, **h1, align,** center, homepage, firefox, implementation  \n",
    "- C#: learning, watching, factor, pickup, animation, fire (maybe used more for modding)  \n",
    "- PHP: addition, kevin, bundle (not too many in training set)  \n",
    "- C: victim, variant, linux, letter, cpu(also used a lot in ruby), memory, block, **sudo** (potentially also more used for video game mods)\n",
    "- SourcePawn: plugins, csgo(name of game), bos, force, sourcemod  \n",
    "- C++: online (not many repos)  \n",
    "- Java: door, plant, sun, storage, private, endpoint, **maven** (likely used for modding)\n",
    "- Python: given, shotgun, exists, specified, dictionary, codeblock, allowed  \n",
    "- Lua: plague, gamemode, language, infection (game mods)  \n",
    "- Ruby: rail, rake, generated, report, csv(?), paragraph, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671796b",
   "metadata": {},
   "source": [
    "### Answer 3: Yes, different programming languages use a different number of unique words.\n",
    "---\n",
    "### Question 4: Does sentiment score vary by programming language?\n",
    "\n",
    "#### $H_{0}$: Average sentiment for languages is about the same.\n",
    "#### $H_{a}$: Average sentiment between languages is significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17adc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kruskal-wallis test for sentiment score\n",
    "explore.run_kruskal_wallis_sentiment(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e919f62",
   "metadata": {},
   "source": [
    "### Answer 4: Yes, there is a significant difference in sentiment score based on programming language.\n",
    "---\n",
    "### Data Exploration Takeaways:\n",
    "\n",
    "- Some of the languages included in our dataset appear to be used primarily for game modding.\n",
    "- Looking at the distribution of frequently occurring words was not helpful.\n",
    "- Not all languages had most frequently occurring words that were unique to only that language, but most did.\n",
    "- There was a significant amount of variation in the length of the README files based on what language they were primarily comprised of.\n",
    "- There was a significant amount of variation in sentiment based on programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb515e82",
   "metadata": {},
   "source": [
    "---\n",
    "## Modeling\n",
    "\n",
    "For our baseline prediction, we decided to use JavaScript. We made this decision because JavaScript appears most frequently in our dataset, along with C#.\n",
    "\n",
    "For all of our models, the features we included were TF-IDF, word count, and character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899370ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create list of features\n",
    "features = train.drop(columns = ['repo','coding_language','readme_contents','repo_link','cleaned_readme','stemmed','lemmatized', 'sentiment']).columns.to_list()\n",
    "\n",
    "# Define X and y for train, validate, and test\n",
    "X_train =  train[features]\n",
    "y_train =  train.coding_language\n",
    "X_validate = validate[features]\n",
    "y_validate = validate.coding_language\n",
    "X_test = test[features]\n",
    "y_test = test.coding_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aeec9",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "- Always predict JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aff2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish baseline\n",
    "train['modeling_baseline'] = 'JavaScript'\n",
    "validate['modeling_baseline'] = 'JavaScript'\n",
    "test['modeling_baseline'] = 'JavaScript'\n",
    "\n",
    "# print results\n",
    "print('Train Accuracy: {:.2%}'.format(accuracy_score(train.coding_language, train.modeling_baseline)))\n",
    "print('\\nValidate Accuracy: {:.2%}'.format(accuracy_score(validate.coding_language, validate.modeling_baseline)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6292328",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression\n",
    "- Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create and fit object\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict for train and validate\n",
    "train['predicted_lm']= lm.predict(X_train)\n",
    "validate['predicted_lm'] = lm.predict(X_validate)\n",
    "\n",
    "# print results\n",
    "print('Train Accuracy: {:.2%}'.format(accuracy_score(train.coding_language, train.predicted_lm)))\n",
    "\n",
    "print('\\nValidate Accuracy: {:.2%}'.format(accuracy_score(validate.coding_language, validate.predicted_lm)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d6ac3",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest\n",
    "- max_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# create random forest object\n",
    "rf = RandomForestClassifier(min_samples_leaf=3,criterion='gini',max_depth=5,random_state=222)\n",
    "\n",
    "# fit model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict on train and validate\n",
    "train['predicted_rf'] = rf.predict(X_train)\n",
    "validate['predicted_rf'] = rf.predict(X_validate)\n",
    "\n",
    "# print results\n",
    "print('Train Accuracy: {:.2%}'.format(accuracy_score(train.coding_language, train.predicted_rf)))\n",
    "print('\\nValidate Accuracy: {:.2%}'.format(accuracy_score(validate.coding_language, validate.predicted_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386fd87",
   "metadata": {},
   "source": [
    "### Model 3: Decision Tree\n",
    "- max_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# create decision tree object\n",
    "clf = DecisionTreeClassifier(max_depth = 5, random_state = 222)\n",
    "# fit object\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on train and validate\n",
    "train['predicted_clf'] = clf.predict(X_train)\n",
    "validate['predicted_clf'] = clf.predict(X_validate)\n",
    "\n",
    "# print results\n",
    "print('Train Accuracy: {:.2%}'.format(accuracy_score(train.coding_language, train.predicted_clf)))\n",
    "print('\\nValidate Accuracy: {:.2%}'.format(accuracy_score(validate.coding_language, validate.predicted_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185fa1a",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- Our Decision Tree model was clearly overfit as it performed better than any other model on train but it had extremely poor performance on validate.\n",
    "- Both of our other models also performed better on train than validate but our Logistic Regression model seemed to be the least overfit as there was less of a difference in accuracy between train and validate.\n",
    "- The Logistic Regression model performed best on the validate sample\n",
    "- Based on the result of the statistical test regarding sentiment score, we initially included sentiment score in the feaures we fed into the model. However, our models performed better on out-of-sample data without including sentiment as a feature in the models.\n",
    "- **Our best model is the Logistic Regression model**\n",
    "\n",
    "### Evaluating the Best Model on Test\n",
    "\n",
    "Since the Logistic Regression model performed best overall, we ran this model on the test sample and compared it to the baseline prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on baseline\n",
    "test['predicted_lm']= lm.predict(X_test)\n",
    "test['predicted_baseline']= 'JavaScript'\n",
    "\n",
    "# print results\n",
    "print('Baseline Accuracy: {:.2%}'.format(accuracy_score(test.coding_language, test.predicted_baseline)))\n",
    "print('Linear Regression Model Accuracy: {:.2%}'.format(accuracy_score(test.coding_language, test.predicted_lm)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9e63d",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "The combination of machine learning and zombie-related repositories did not provide for very useful predictions. With a Logistic Regression model, we were able to achieve 13.5% accuracy in our predictions on unseen data. While this did beat the baseline, we know it could be greatly improved.\n",
    "\n",
    "### Next Steps\n",
    "With more time we would like to train our models using more repositories with a greater variety of subjects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
